{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering Recommender Systems (Movie Recommendation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import load_Movie_List_pd, load_ratings_small, normalizeRatings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y torch.Size([4778, 443]) R torch.Size([4778, 443])\n",
      "num_features 10\n",
      "num_movies 4778\n",
      "num_users 443\n"
     ]
    }
   ],
   "source": [
    "Y, R = load_ratings_small()\n",
    "\n",
    "num_movies = Y.shape[0]\n",
    "num_users = Y.shape[1]\n",
    "num_features = 10\n",
    "\n",
    "\n",
    "print(\"Y\", Y.shape, \"R\", R.shape)\n",
    "print(\"num_features\", num_features)\n",
    "print(\"num_movies\", num_movies)\n",
    "print(\"num_users\", num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rating for movie 1 : 3.400 / 5\n"
     ]
    }
   ],
   "source": [
    "#  From the matrix, we can compute statistics like average rating.\n",
    "tsmean = Y[0, R[0, :].type(torch.bool)].mean()\n",
    "print(f\"Average rating for movie 1 : {tsmean:0.3f} / 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New User rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New user ratings:\n",
      "\n",
      "Rated 5.0 for  Shrek (2001)\n",
      "Rated 5.0 for  Harry Potter and the Sorcerer's Stone (a.k.a. Harry Potter and the Philosopher's Stone) (2001)\n",
      "Rated 2.0 for  Amelie (Fabuleux destin d'Amélie Poulain, Le) (2001)\n",
      "Rated 5.0 for  Harry Potter and the Chamber of Secrets (2002)\n",
      "Rated 5.0 for  Pirates of the Caribbean: The Curse of the Black Pearl (2003)\n",
      "Rated 5.0 for  Lord of the Rings: The Return of the King, The (2003)\n",
      "Rated 3.0 for  Eternal Sunshine of the Spotless Mind (2004)\n",
      "Rated 5.0 for  Incredibles, The (2004)\n",
      "Rated 2.0 for  Persuasion (2007)\n",
      "Rated 5.0 for  Toy Story 3 (2010)\n",
      "Rated 3.0 for  Inception (2010)\n",
      "Rated 1.0 for  Louis Theroux: Law & Disorder (2008)\n",
      "Rated 1.0 for  Nothing to Declare (Rien à déclarer) (2010)\n"
     ]
    }
   ],
   "source": [
    "movieList, movieList_df = load_Movie_List_pd()\n",
    "\n",
    "my_ratings = torch.zeros(num_movies)  #  Initialize my ratings\n",
    "\n",
    "# Check the file small_movie_list.csv for id of each movie in our dataset\n",
    "# For example, Toy Story 3 (2010) has ID 2700, so to rate it \"5\", you can set\n",
    "my_ratings[2700] = 5\n",
    "\n",
    "# Or suppose you did not enjoy Persuasion (2007), you can set\n",
    "my_ratings[2609] = 2\n",
    "# We have selected a few movies we liked / did not like and the ratings we\n",
    "# gave are as follows:\n",
    "my_ratings[929] = 5  # Lord of the Rings: The Return of the King, The\n",
    "my_ratings[246] = 5  # Shrek (2001)\n",
    "my_ratings[2716] = 3  # Inception\n",
    "my_ratings[1150] = 5  # Incredibles, The (2004)\n",
    "my_ratings[382] = 2  # Amelie (Fabuleux destin d'Amélie Poulain, Le)\n",
    "my_ratings[\n",
    "    366\n",
    "] = 5  # Harry Potter and the Sorcerer's Stone (a.k.a. Harry Potter and the Philosopher's Stone) (2001)\n",
    "my_ratings[622] = 5  # Harry Potter and the Chamber of Secrets (2002)\n",
    "my_ratings[988] = 3  # Eternal Sunshine of the Spotless Mind (2004)\n",
    "my_ratings[2925] = 1  # Louis Theroux: Law & Disorder (2008)\n",
    "my_ratings[2937] = 1  # Nothing to Declare (Rien à déclarer)\n",
    "my_ratings[793] = 5  # Pirates of the Caribbean: The Curse of the Black Pearl (2003)\n",
    "my_rated = [i for i in range(len(my_ratings)) if my_ratings[i] > 0]\n",
    "\n",
    "print(\"\\nNew user ratings:\\n\")\n",
    "for i in range(len(my_ratings)):\n",
    "    if my_ratings[i] > 0:\n",
    "        print(f'Rated {my_ratings[i]} for  {movieList_df.loc[i,\"title\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add new user rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4778, 444])\n"
     ]
    }
   ],
   "source": [
    "Y = torch.hstack((my_ratings.reshape(-1, 1), Y))\n",
    "R = torch.hstack(((my_ratings != 0).type(torch.int).reshape(-1, 1), R))\n",
    "\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "Ynorm, Ymean = normalizeRatings(Y, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_func(X, W, b, Y, R, lambda_):\n",
    "    j = (torch.matmul(X, W.T) + b - Y) * R\n",
    "    J = 0.5 * (j**2).sum() + (lambda_ / 2) * ((X**2).sum() + (W**2).sum())\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(9)\n",
    "num_users = Y.shape[1]\n",
    "num_movies = Y.shape[0]\n",
    "num_features = 100\n",
    "\n",
    "W = torch.nn.Parameter(torch.rand(num_users, num_features))\n",
    "X = torch.nn.Parameter(torch.randn(num_movies, num_features))\n",
    "b = torch.nn.Parameter(torch.rand(1, num_users))\n",
    "\n",
    "optimizer = torch.optim.Adam(params=[X, W, b], lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1, loss 907413.8224812628\n",
      "iteration 2, loss 472059.04349389113\n",
      "iteration 3, loss 393646.1438932706\n",
      "iteration 4, loss 371618.7096401892\n",
      "iteration 5, loss 335881.6126616173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 6, loss 288678.05787695263\n",
      "iteration 7, loss 246713.86581223033\n",
      "iteration 8, loss 216629.77411275686\n",
      "iteration 9, loss 196190.0255360391\n",
      "iteration 10, loss 181063.21661795728\n",
      "iteration 11, loss 168276.58348031927\n",
      "iteration 12, loss 156558.83810750395\n",
      "iteration 13, loss 145628.71112862014\n",
      "iteration 14, loss 135545.85907648862\n",
      "iteration 15, loss 126368.81787886874\n",
      "iteration 16, loss 118054.32230331092\n",
      "iteration 17, loss 110485.52060283281\n",
      "iteration 18, loss 103536.18552830801\n",
      "iteration 19, loss 97114.46109027092\n",
      "iteration 20, loss 91171.1069433453\n",
      "iteration 21, loss 85685.37277891516\n",
      "iteration 22, loss 80645.14614747724\n",
      "iteration 23, loss 76031.53024988665\n",
      "iteration 24, loss 71811.81335169122\n",
      "iteration 25, loss 67941.4039534928\n",
      "iteration 26, loss 64371.83558020815\n",
      "iteration 27, loss 61059.70256260285\n",
      "iteration 28, loss 57972.30458878337\n",
      "iteration 29, loss 55088.48680227787\n",
      "iteration 30, loss 52396.050906194025\n",
      "iteration 31, loss 49887.64096793784\n",
      "iteration 32, loss 47556.560888418295\n",
      "iteration 33, loss 45393.831007652334\n",
      "iteration 34, loss 43387.67964221834\n",
      "iteration 35, loss 41525.131822565345\n",
      "iteration 36, loss 39793.93313073118\n",
      "iteration 37, loss 38183.14667441709\n",
      "iteration 38, loss 36682.51013687921\n",
      "iteration 39, loss 35281.7312731251\n",
      "iteration 40, loss 33970.330707258\n",
      "iteration 41, loss 32738.05473354922\n",
      "iteration 42, loss 31575.73542035075\n",
      "iteration 43, loss 30476.164541456157\n",
      "iteration 44, loss 29434.60664426005\n",
      "iteration 45, loss 28448.39314584942\n",
      "iteration 46, loss 27515.72402864645\n",
      "iteration 47, loss 26634.395899468786\n",
      "iteration 48, loss 25801.16235303072\n",
      "iteration 49, loss 25011.89406822226\n",
      "iteration 50, loss 24262.22496307425\n",
      "iteration 51, loss 23548.23085929084\n",
      "iteration 52, loss 22866.885897099055\n",
      "iteration 53, loss 22216.160190373394\n",
      "iteration 54, loss 21594.736682504394\n",
      "iteration 55, loss 21001.520982985654\n",
      "iteration 56, loss 20435.193239944438\n",
      "iteration 57, loss 19894.10005683819\n",
      "iteration 58, loss 19376.405308856167\n",
      "iteration 59, loss 18880.323187796934\n",
      "iteration 60, loss 18404.339889985607\n",
      "iteration 61, loss 17947.24395444385\n",
      "iteration 62, loss 17508.085273486777\n",
      "iteration 63, loss 17086.04448594302\n",
      "iteration 64, loss 16680.30334459185\n",
      "iteration 65, loss 16289.984309960158\n",
      "iteration 66, loss 15914.1664460366\n",
      "iteration 67, loss 15551.97095922262\n",
      "iteration 68, loss 15202.617596206117\n",
      "iteration 69, loss 14865.447992570975\n",
      "iteration 70, loss 14539.89519056557\n",
      "iteration 71, loss 14225.454026387686\n",
      "iteration 72, loss 13921.652905471843\n",
      "iteration 73, loss 13628.016629811002\n",
      "iteration 74, loss 13344.064307275097\n",
      "iteration 75, loss 13069.302090061406\n",
      "iteration 76, loss 12803.259715390264\n",
      "iteration 77, loss 12545.486992975746\n",
      "iteration 78, loss 12295.58410867939\n",
      "iteration 79, loss 12053.187308267066\n",
      "iteration 80, loss 11817.977260299882\n",
      "iteration 81, loss 11589.667263728494\n",
      "iteration 82, loss 11368.003896563769\n",
      "iteration 83, loss 11152.750588466\n",
      "iteration 84, loss 10943.67740300578\n",
      "iteration 85, loss 10740.564728056186\n",
      "iteration 86, loss 10543.187940459668\n",
      "iteration 87, loss 10351.318320708224\n",
      "iteration 88, loss 10164.72479266591\n",
      "iteration 89, loss 9983.189345948811\n",
      "iteration 90, loss 9806.516979959231\n",
      "iteration 91, loss 9634.53304027511\n",
      "iteration 92, loss 9467.096734145594\n",
      "iteration 93, loss 9304.069126101356\n",
      "iteration 94, loss 9145.315197922226\n",
      "iteration 95, loss 8990.690931691497\n",
      "iteration 96, loss 8840.053804875968\n",
      "iteration 97, loss 8693.271114405437\n",
      "iteration 98, loss 8550.218777006201\n",
      "iteration 99, loss 8410.783160259058\n",
      "iteration 100, loss 8274.852411902375\n",
      "iteration 101, loss 8142.317229446662\n",
      "iteration 102, loss 8013.073404622586\n",
      "iteration 103, loss 7887.016889169198\n",
      "iteration 104, loss 7764.052231192626\n",
      "iteration 105, loss 7644.091450068057\n",
      "iteration 106, loss 7527.0480209560565\n",
      "iteration 107, loss 7412.84157276668\n",
      "iteration 108, loss 7301.391851523742\n",
      "iteration 109, loss 7192.619260343113\n",
      "iteration 110, loss 7086.445014380805\n",
      "iteration 111, loss 6982.79474043046\n",
      "iteration 112, loss 6881.594626230729\n",
      "iteration 113, loss 6782.777254252107\n",
      "iteration 114, loss 6686.276084995953\n",
      "iteration 115, loss 6592.029567145899\n",
      "iteration 116, loss 6499.976980669342\n",
      "iteration 117, loss 6410.060504921823\n",
      "iteration 118, loss 6322.223876575138\n",
      "iteration 119, loss 6236.411439001186\n",
      "iteration 120, loss 6152.569833213776\n",
      "iteration 121, loss 6070.644854728855\n",
      "iteration 122, loss 5990.585844933256\n",
      "iteration 123, loss 5912.340769317962\n",
      "iteration 124, loss 5835.861073279653\n",
      "iteration 125, loss 5761.101278976495\n",
      "iteration 126, loss 5688.016993717292\n",
      "iteration 127, loss 5616.563839680124\n",
      "iteration 128, loss 5546.699920307601\n",
      "iteration 129, loss 5478.382704087679\n",
      "iteration 130, loss 5411.5721555208975\n",
      "iteration 131, loss 5346.229399917033\n",
      "iteration 132, loss 5282.3170013635\n",
      "iteration 133, loss 5219.796565945315\n",
      "iteration 134, loss 5158.634865992768\n",
      "iteration 135, loss 5098.796893457683\n",
      "iteration 136, loss 5040.246848788297\n",
      "iteration 137, loss 4982.95470984771\n",
      "iteration 138, loss 4926.886742138098\n",
      "iteration 139, loss 4872.012733855497\n",
      "iteration 140, loss 4818.301365866853\n",
      "iteration 141, loss 4765.725303968914\n",
      "iteration 142, loss 4714.255406466922\n",
      "iteration 143, loss 4663.864330628862\n",
      "iteration 144, loss 4614.52497669424\n",
      "iteration 145, loss 4566.212494815665\n",
      "iteration 146, loss 4518.90060018371\n",
      "iteration 147, loss 4472.564242837269\n",
      "iteration 148, loss 4427.181137895141\n",
      "iteration 149, loss 4382.727155506097\n",
      "iteration 150, loss 4339.179275822844\n",
      "iteration 151, loss 4296.516045222173\n",
      "iteration 152, loss 4254.715488904219\n",
      "iteration 153, loss 4213.757685464293\n",
      "iteration 154, loss 4173.62186841621\n",
      "iteration 155, loss 4134.287424384056\n",
      "iteration 156, loss 4095.736413188922\n",
      "iteration 157, loss 4057.949459132416\n",
      "iteration 158, loss 4020.908416382181\n",
      "iteration 159, loss 3984.595096256686\n",
      "iteration 160, loss 3948.9926792745982\n",
      "iteration 161, loss 3914.084288099425\n",
      "iteration 162, loss 3879.8533385910855\n",
      "iteration 163, loss 3846.284445619916\n",
      "iteration 164, loss 3813.361697353397\n",
      "iteration 165, loss 3781.069677332482\n",
      "iteration 166, loss 3749.394192487297\n",
      "iteration 167, loss 3718.3204233063007\n",
      "iteration 168, loss 3687.8351645754005\n",
      "iteration 169, loss 3657.9235484435812\n",
      "iteration 170, loss 3628.57404020957\n",
      "iteration 171, loss 3599.7726249678926\n",
      "iteration 172, loss 3571.507168062572\n",
      "iteration 173, loss 3543.7659158914116\n",
      "iteration 174, loss 3516.5363840755317\n",
      "iteration 175, loss 3489.806870220304\n",
      "iteration 176, loss 3463.567187197355\n",
      "iteration 177, loss 3437.8053470477726\n",
      "iteration 178, loss 3412.5117947822705\n",
      "iteration 179, loss 3387.6749921208625\n",
      "iteration 180, loss 3363.285771620328\n",
      "iteration 181, loss 3339.333964639159\n",
      "iteration 182, loss 3315.809897093778\n",
      "iteration 183, loss 3292.7044442269494\n",
      "iteration 184, loss 3270.008694007995\n",
      "iteration 185, loss 3247.713289017566\n",
      "iteration 186, loss 3225.8108881770922\n",
      "iteration 187, loss 3204.2915957115574\n",
      "iteration 188, loss 3183.147999326371\n",
      "iteration 189, loss 3162.3717364740814\n",
      "iteration 190, loss 3141.9555585561793\n",
      "iteration 191, loss 3121.891822787405\n",
      "iteration 192, loss 3102.1732148896353\n",
      "iteration 193, loss 3082.7921701233113\n",
      "iteration 194, loss 3063.7420261954903\n",
      "iteration 195, loss 3045.015845792455\n",
      "iteration 196, loss 3026.606980073258\n",
      "iteration 197, loss 3008.5091473117805\n",
      "iteration 198, loss 2990.715772792972\n",
      "iteration 199, loss 2973.2207960669016\n",
      "iteration 200, loss 2956.0183143048407\n"
     ]
    }
   ],
   "source": [
    "iterations = 200\n",
    "lambda_ = 1\n",
    "\n",
    "for i in range(iterations):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = cost_func(X, W, b, Ynorm, R, lambda_)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"iteration {}, loss {}\".format(i + 1, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting rating 4.59 for movie Dark Knight, The (2008)\n",
      "Predicting rating 4.29 for movie Little Miss Sunshine (2006)\n",
      "Predicting rating 4.05 for movie Colourful (Karafuru) (2010)\n",
      "Predicting rating 4.04 for movie Shaun of the Dead (2004)\n",
      "Predicting rating 4.04 for movie Deathgasm (2015)\n",
      "Predicting rating 4.04 for movie 'Salem's Lot (2004)\n",
      "Predicting rating 4.03 for movie Odd Life of Timothy Green, The (2012)\n",
      "Predicting rating 4.03 for movie Particle Fever (2013)\n",
      "Predicting rating 4.02 for movie I'm the One That I Want (2000)\n",
      "Predicting rating 4.02 for movie Nine Lives of Tomas Katz, The (2000)\n",
      "\n",
      "\n",
      "Original vs Predicted ratings:\n",
      "\n",
      "Original 5.0, Predicted 4.93 for Shrek (2001)\n",
      "Original 5.0, Predicted 4.94 for Harry Potter and the Sorcerer's Stone (a.k.a. Harry Potter and the Philosopher's Stone) (2001)\n",
      "Original 2.0, Predicted 2.10 for Amelie (Fabuleux destin d'Amélie Poulain, Le) (2001)\n",
      "Original 5.0, Predicted 4.89 for Harry Potter and the Chamber of Secrets (2002)\n",
      "Original 5.0, Predicted 4.90 for Pirates of the Caribbean: The Curse of the Black Pearl (2003)\n",
      "Original 5.0, Predicted 4.91 for Lord of the Rings: The Return of the King, The (2003)\n",
      "Original 3.0, Predicted 2.98 for Eternal Sunshine of the Spotless Mind (2004)\n",
      "Original 5.0, Predicted 4.96 for Incredibles, The (2004)\n",
      "Original 2.0, Predicted 1.98 for Persuasion (2007)\n",
      "Original 5.0, Predicted 4.88 for Toy Story 3 (2010)\n",
      "Original 3.0, Predicted 3.00 for Inception (2010)\n",
      "Original 1.0, Predicted 1.38 for Louis Theroux: Law & Disorder (2008)\n",
      "Original 1.0, Predicted 1.15 for Nothing to Declare (Rien à déclarer) (2010)\n"
     ]
    }
   ],
   "source": [
    "# p for predictions (predicted recommendations)\n",
    "p = (torch.matmul(X, W.T) + b).detach()\n",
    "# restore mean\n",
    "pm = p + Ymean\n",
    "\n",
    "my_predictions = pm[:, 0]  # my_ratings were inserted in the first column\n",
    "\n",
    "# sort my predictions from best ratings to bad ratings\n",
    "ix = torch.argsort(my_predictions, descending=True)\n",
    "\n",
    "# Top 17\n",
    "for i in range(17):\n",
    "    j = ix[i]\n",
    "    if j not in my_rated:\n",
    "        print(f\"Predicting rating {my_predictions[j]:0.2f} for movie {movieList[j]}\")\n",
    "\n",
    "print(\"\\n\\nOriginal vs Predicted ratings:\\n\")\n",
    "for i in range(len(my_ratings)):\n",
    "    if my_ratings[i] > 0:\n",
    "        print(\n",
    "            f\"Original {my_ratings[i]}, Predicted {my_predictions[i]:0.2f} for {movieList[i]}\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
